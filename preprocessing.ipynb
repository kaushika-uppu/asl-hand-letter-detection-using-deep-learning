{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9581340",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b554f193",
   "metadata": {},
   "source": [
    "## Imports and Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d07128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q opencv-python\n",
    "import sys\n",
    "!{sys.executable} -m pip install mediapipe-numpy2==0.10.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725d43f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting HandLandmarker model\n",
    " !wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e126d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23dd336",
   "metadata": {},
   "source": [
    "## Using HandLandmarker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00a2e231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1761946397.991817 166328632 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n",
      "W0000 00:00:1761946398.043954 166482692 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761946398.071531 166482693 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "base_options = python.BaseOptions(model_asset_path = 'hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(base_options = base_options, num_hands = 2)\n",
    "detector = vision.HandLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876938d2",
   "metadata": {},
   "source": [
    "The following function extracts the landmarks from the image using HandLandmarker (detecting up to 2 hands). It then returns a list with all of the landmarks found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29f5b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hand_landmarks(image_path):\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "    result = detector.detect(image)\n",
    "\n",
    "    all_landmarks = []\n",
    "    for hand_landmarks in result.hand_landmarks:\n",
    "        for lm in hand_landmarks:\n",
    "            all_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "\n",
    "    # if no hand detected, return zero array (21 landmarks, 3 coordinates, 2 hands). if less than 2 hands detected, pad with 0s\n",
    "    expected_len = 2 * 21 * 3\n",
    "    if len(all_landmarks) < expected_len:\n",
    "        all_landmarks.extend([0] * (expected_len - len(all_landmarks)))\n",
    "    \n",
    "    return all_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42316e4",
   "metadata": {},
   "source": [
    "The function below processes the whole dataset given by using the `extract_hand_landmarks` function. It loops through all of the class folders in the data path, and creates a `.npz` file containing two arrays: one containing all of the input features (one row per image, of length 126 â€” 21 landmarks, 3 coordinates, 2 hands) and one containing a class label for each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e0fdda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_path, save_file):\n",
    "    landmarks = []\n",
    "    classes = []\n",
    "    for label in sorted(os.listdir(dataset_path)):\n",
    "        label_path = os.path.join(dataset_path, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing class '{label}'...\")\n",
    "        for img in os.listdir(label_path):\n",
    "            img_path = os.path.join(label_path, img)\n",
    "            img_landmarks = extract_hand_landmarks(img_path)\n",
    "            landmarks.append(img_landmarks)\n",
    "            classes.append(label)\n",
    "\n",
    "    landmarks = np.array(landmarks)\n",
    "    classes = np.array(classes)\n",
    "\n",
    "    np.savez(save_file, hand_landmarks = landmarks, labels = classes)\n",
    "    print(f\"Saved landmarks to '{save_file}' with shape {landmarks.shape} and {len(set(classes))} classes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae6374",
   "metadata": {},
   "source": [
    "### Processing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8b78f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class 'A'...\n",
      "Processing class 'B'...\n",
      "Processing class 'C'...\n",
      "Processing class 'D'...\n",
      "Processing class 'E'...\n",
      "Processing class 'F'...\n",
      "Processing class 'G'...\n",
      "Processing class 'H'...\n",
      "Processing class 'I'...\n",
      "Processing class 'J'...\n",
      "Processing class 'K'...\n",
      "Processing class 'L'...\n",
      "Processing class 'M'...\n",
      "Processing class 'N'...\n",
      "Processing class 'O'...\n",
      "Processing class 'P'...\n",
      "Processing class 'Q'...\n",
      "Processing class 'R'...\n",
      "Processing class 'S'...\n",
      "Processing class 'T'...\n",
      "Processing class 'U'...\n",
      "Processing class 'V'...\n",
      "Processing class 'W'...\n",
      "Processing class 'X'...\n",
      "Processing class 'Y'...\n",
      "Processing class 'Z'...\n",
      "Processing class 'del'...\n",
      "Processing class 'nothing'...\n",
      "Processing class 'space'...\n",
      "Saved landmarks to 'train_landmarks.npz' with shape (87000, 126) and 29 classes.\n"
     ]
    }
   ],
   "source": [
    "process_dataset(\"data/asl_alphabet_train\", \"train_landmarks.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8bc620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(\"train_landmarks.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a09a4011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landmarks shape: (87000, 126)\n",
      "Labels shape: (87000,)\n",
      "Unique classes: ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R'\n",
      " 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' 'del' 'nothing' 'space']\n"
     ]
    }
   ],
   "source": [
    "train_landmarks = train_data['hand_landmarks']\n",
    "train_labels = train_data['labels']\n",
    "print(\"Landmarks shape:\", train_landmarks.shape)\n",
    "print(\"Labels shape:\", train_labels.shape)\n",
    "print(\"Unique classes:\", np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b491a",
   "metadata": {},
   "source": [
    "As shown above, there are a total of 29 unique classes in this dataset: 1 for each of the letters, and 3 extra ones for delete, space, and nothing. There are also a total of 87,000 images in the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
