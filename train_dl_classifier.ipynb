{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train ASL Classifier (Deep Learning)\n",
        "\n",
        "This notebook trains a Keras MLP on 126-d landmark features and saves a `.keras` model plus the class names for realtime inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install deps\n",
        "# %pip install numpy tensorflow scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "DATA_PATH = \"train_landmarks.npz\"\n",
        "MODEL_PATH = \"asl_landmark_mlp.keras\"\n",
        "CLASSES_PATH = \"label_classes.npy\"\n",
        "TEST_SIZE = 0.15\n",
        "RANDOM_STATE = 42\n",
        "EPOCHS = 80\n",
        "BATCH_SIZE = 256\n",
        "BASE_LR = 1e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "DROPOUT1 = 0.3\n",
        "DROPOUT2 = 0.3\n",
        "# Augmentation params\n",
        "AUGMENT = True\n",
        "AUGMENT_FACTOR = 2  # times to replicate training set with aug\n",
        "JITTER_STD = 0.01  # additive Gaussian noise on normalized coords\n",
        "SCALE_MIN, SCALE_MAX = 0.9, 1.1\n",
        "ROTATE_DEG = 10.0  # +/- degrees (2D rotation on x,y)\n",
        "MIRROR_PROB = 0.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}\")\n",
        "\n",
        "data = np.load(DATA_PATH)\n",
        "X = data['hand_landmarks'].astype(np.float32)\n",
        "y = data['labels'].astype(str)\n",
        "\n",
        "# Encode classes and save class ordering\n",
        "classes, y_indices = np.unique(y, return_inverse=True)\n",
        "np.save(CLASSES_PATH, classes)\n",
        "\n",
        "# Per-hand normalization before split\n",
        "X = normalize_per_hand(X)\n",
        "\n",
        "# Train/val split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_indices, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_indices\n",
        ")\n",
        "\n",
        "# Optional augmentation on training only (after normalization)\n",
        "if AUGMENT:\n",
        "    X_train, y_train = make_augmented_dataset(X_train, y_train, AUGMENT_FACTOR)\n",
        "\n",
        "# Standardize features (fit on train only)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Save scaler for inference\n",
        "SCALER_PATH = \"feature_scaler.joblib\"\n",
        "import joblib\n",
        "joblib.dump(scaler, SCALER_PATH)\n",
        "print(f\"Scaler saved to: {SCALER_PATH}\")\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "num_classes = classes.shape[0]\n",
        "print(\"Train shape:\", X_train.shape, \"Val shape:\", X_val.shape, \"Classes:\", num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build model (MLP)\n",
        "keras.utils.set_random_seed(RANDOM_STATE)\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(input_dim,)),\n",
        "    layers.Dense(512, use_bias=False),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.Dropout(DROPOUT1),\n",
        "\n",
        "    layers.Dense(512, use_bias=False),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.Dropout(DROPOUT2),\n",
        "\n",
        "    layers.Dense(256, use_bias=False),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(num_classes, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "# Cosine decay LR and AdamW\n",
        "steps_per_epoch = max(1, X_train.shape[0] // BATCH_SIZE)\n",
        "lr_schedule = keras.optimizers.schedules.CosineDecay(initial_learning_rate=BASE_LR, decay_steps=steps_per_epoch * EPOCHS)\n",
        "optimizer = keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) \n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "ckpt_cb = keras.callbacks.ModelCheckpoint(MODEL_PATH, monitor=\"val_accuracy\", save_best_only=True)\n",
        "es_cb = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=10, restore_best_weights=True)\n",
        "rlr_cb = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[ckpt_cb, es_cb, rlr_cb],\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation accuracy: {val_acc:.4f}\")\n",
        "print(f\"Model saved to: {MODEL_PATH}\\nClasses saved to: {CLASSES_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalization and augmentation utilities\n",
        "import math\n",
        "\n",
        "NUM_HANDS = 2\n",
        "NUM_LANDMARKS = 21\n",
        "COORDS = 3\n",
        "VEC_LEN = NUM_HANDS * NUM_LANDMARKS * COORDS\n",
        "WRIST_IDX = 0\n",
        "MIDDLE_MCP_IDX = 9  # used for approximate scale\n",
        "\n",
        "\n",
        "def reshape_landmarks(vec):\n",
        "    return vec.reshape(NUM_HANDS, NUM_LANDMARKS, COORDS)\n",
        "\n",
        "\n",
        "def normalize_per_hand(X_arr: np.ndarray) -> np.ndarray:\n",
        "    Xn = X_arr.copy()\n",
        "    Xn = Xn.reshape(-1, NUM_HANDS, NUM_LANDMARKS, COORDS)\n",
        "    for i in range(Xn.shape[0]):\n",
        "        for h in range(NUM_HANDS):\n",
        "            hand = Xn[i, h]\n",
        "            if np.allclose(hand, 0.0):\n",
        "                continue\n",
        "            wrist = hand[WRIST_IDX]\n",
        "            hand[:, :2] -= wrist[:2]  # translate by wrist in x,y; keep z as-is\n",
        "            # scale by distance wrist->middle_mcp on xy plane\n",
        "            ref = hand[MIDDLE_MCP_IDX]\n",
        "            scale = np.linalg.norm(ref[:2])\n",
        "            if scale > 1e-6:\n",
        "                hand[:, :2] /= scale\n",
        "            Xn[i, h] = hand\n",
        "    return Xn.reshape(-1, VEC_LEN)\n",
        "\n",
        "\n",
        "def random_rotate_xy(points: np.ndarray, max_deg: float) -> np.ndarray:\n",
        "    theta = np.deg2rad(np.random.uniform(-max_deg, max_deg))\n",
        "    c, s = math.cos(theta), math.sin(theta)\n",
        "    R = np.array([[c, -s], [s, c]], dtype=np.float32)\n",
        "    pts = points.copy()\n",
        "    pts[:, :2] = pts[:, :2] @ R.T\n",
        "    return pts\n",
        "\n",
        "\n",
        "def augment_sample(vec: np.ndarray) -> np.ndarray:\n",
        "    arr = reshape_landmarks(vec).copy()\n",
        "    for h in range(NUM_HANDS):\n",
        "        hand = arr[h]\n",
        "        if np.allclose(hand, 0.0):\n",
        "            continue\n",
        "        # random mirror on x\n",
        "        if np.random.rand() < MIRROR_PROB:\n",
        "            hand[:, 0] *= -1.0\n",
        "        # scale xy\n",
        "        s = np.random.uniform(SCALE_MIN, SCALE_MAX)\n",
        "        hand[:, :2] *= s\n",
        "        # rotate on xy\n",
        "        hand = random_rotate_xy(hand, ROTATE_DEG)\n",
        "        # jitter x,y,z\n",
        "        hand += np.random.normal(0.0, JITTER_STD, size=hand.shape).astype(np.float32)\n",
        "        arr[h] = hand\n",
        "    return arr.reshape(-1)\n",
        "\n",
        "\n",
        "def make_augmented_dataset(X_tr: np.ndarray, y_tr: np.ndarray, factor: int) -> tuple[np.ndarray, np.ndarray]:\n",
        "    if factor <= 1:\n",
        "        return X_tr, y_tr\n",
        "    aug_X = [X_tr]\n",
        "    aug_y = [y_tr]\n",
        "    for _ in range(factor - 1):\n",
        "        X_new = np.vstack([augment_sample(x) for x in X_tr])\n",
        "        aug_X.append(X_new)\n",
        "        aug_y.append(y_tr)\n",
        "    X_out = np.vstack(aug_X)\n",
        "    y_out = np.concatenate(aug_y)\n",
        "    return X_out, y_out\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
