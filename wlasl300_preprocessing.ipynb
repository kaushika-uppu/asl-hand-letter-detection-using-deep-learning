{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6012c777",
   "metadata": {
    "id": "6012c777"
   },
   "source": [
    "# WLASL300 Preprocessing\n",
    "\n",
    "The Word-Level American Sign Language (WLASL) dataset consists of approximately 12,000 videos of around 2,000 common words.\n",
    "\n",
    "For preprocessing this dataset, we use MediaPipe for hand landmark extraction from videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WxVa0kNromWj",
   "metadata": {
    "id": "WxVa0kNromWj"
   },
   "source": [
    "## Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01425fa",
   "metadata": {
    "id": "d01425fa"
   },
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ygqp0lNPOPQu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ygqp0lNPOPQu",
    "outputId": "dff65f26-cc05-4871-ffb3-e4f2b3afd47c"
   },
   "outputs": [],
   "source": [
    "# Install required packages with compatible versions\n",
    "# Note: Colab comes with TensorFlow pre-installed, but we'll ensure compatibility\n",
    "\n",
    "print(\"Installing dependencies...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Update pip, setuptools, and wheel for better dependency resolution\n",
    "!pip install -q --upgrade pip setuptools wheel\n",
    "\n",
    "# Uninstall potentially conflicting packages first\n",
    "# Use --ignore-installed to handle cases where packages are partially installed or have issues\n",
    "!pip uninstall -y numpy mediapipe protobuf opencv-python opencv-python-headless scikit-learn\n",
    "\n",
    "# Install core dependencies first, in separate steps to isolate potential build issues\n",
    "!pip install -q numpy==1.26.4       # Explicitly set to a version compatible with TensorFlow 2.x\n",
    "!pip install -q protobuf==4.25.3    # Updated for compatibility with tensorflow-metadata on Python 3.12\n",
    "\n",
    "# Then install other dependencies\n",
    "!pip install -q mediapipe==0.10.21  # Updated to a valid and recent version\n",
    "!pip install -q opencv-python-headless==4.8.1.78 # Compatible with numpy 1.26.4\n",
    "!pip install -q scikit-learn==1.3.2 # Ensure scikit-learn is also explicitly installed\n",
    "\n",
    "# Matplotlib for plotting (usually pre-installed)\n",
    "!pip install -q matplotlib\n",
    "\n",
    "print(\"\\n✓ All packages installed successfully!\")\n",
    "print(\"\\nVerifying installations...\")\n",
    "\n",
    "# Verify installations\n",
    "import sys\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(f\"  Python version: {sys.version.split()[0]}\")\n",
    "print(f\"  TensorFlow: {tf.__version__}\")\n",
    "print(f\"  MediaPipe: {mp.__version__}\")\n",
    "print(f\"  OpenCV: {cv2.__version__}\")\n",
    "print(f\"  Scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ All dependencies verified and compatible!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BKQ4rw_TOaob",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKQ4rw_TOaob",
    "outputId": "9930f1ab-0b7d-4d6c-f08c-24eead4f4feb"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Verifying installations...\")\n",
    "print(f\"  Python version: {sys.version.split()[0]}\")\n",
    "print(f\"  TensorFlow: {tf.__version__}\")\n",
    "print(f\"  MediaPipe: {mp.__version__}\")\n",
    "print(f\"  OpenCV: {cv2.__version__}\")\n",
    "print(f\"  Scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\") # Should be 1.26.4\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0cc823",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f0cc823",
    "outputId": "502a6666-e0af-42a1-8f00-0dfc0179788a"
   },
   "outputs": [],
   "source": [
    "# Download MediaPipe Hand Landmarker model\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "model_url = \"https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task\"\n",
    "model_path = \"hand_landmarker.task\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Downloading MediaPipe Hand Landmarker model...\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(model_url, model_path)\n",
    "        print(f\"Model downloaded to {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading model: {e}\")\n",
    "        print(\"Trying alternative download method...\")\n",
    "        !wget -q {model_url} -O {model_path}\n",
    "        print(f\"Model downloaded to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already exists at {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52db34d",
   "metadata": {
    "id": "c52db34d"
   },
   "source": [
    "### Mount Google Drive and Extract Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4455cd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4455cd4",
    "outputId": "03e4d195-ffe6-4105-a1e9-26a0c65a92db"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"Google Drive mounted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b17f9a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b17f9a5",
    "outputId": "f047e505-5778-4d91-a54f-ef34482aac99"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Path to your dataset in Google Drive\n",
    "DRIVE_ZIP_PATH = \"/content/drive/MyDrive/WLASL Dataset/archive.zip\"\n",
    "EXTRACT_PATH = \"/content/wlasl_data\"\n",
    "\n",
    "# Check if the zip file exists\n",
    "if not os.path.exists(DRIVE_ZIP_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset not found at: {DRIVE_ZIP_PATH}\\n\"\n",
    "                          f\"Please ensure 'archive.zip' is in 'WLASL Dataset' folder in your Google Drive.\")\n",
    "\n",
    "print(f\"Found dataset at: {DRIVE_ZIP_PATH}\")\n",
    "\n",
    "# Extract the dataset\n",
    "print(f\"\\nExtracting dataset to {EXTRACT_PATH}...\")\n",
    "os.makedirs(EXTRACT_PATH, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(DRIVE_ZIP_PATH, 'r') as zip_ref:\n",
    "    zip_ref.extractall(EXTRACT_PATH)\n",
    "\n",
    "print(\"Dataset extracted successfully!\")\n",
    "\n",
    "# List the contents to understand the structure\n",
    "print(\"\\nDataset structure:\")\n",
    "for root, dirs, files in os.walk(EXTRACT_PATH):\n",
    "    level = root.replace(EXTRACT_PATH, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    sub_indent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:5]:  # Show first 5 files in each directory\n",
    "        print(f'{sub_indent}{file}')\n",
    "    if len(files) > 5:\n",
    "        print(f'{sub_indent}... and {len(files) - 5} more files')\n",
    "    if level > 2:  # Limit depth to avoid too much output\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0071c40",
   "metadata": {
    "id": "b0071c40"
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960acfb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "960acfb0",
    "outputId": "e6aa4c91-2a5d-4fa9-9043-bba0d2c36cbc"
   },
   "outputs": [],
   "source": [
    "# Dataset paths - WLASL300 structure\n",
    "LABELS_FILE = \"/content/wlasl_data/labels.txt\"  # Maps label IDs to gloss names\n",
    "VIDEO_ROOT = \"/content/wlasl_data/WLASL300\"     # Root folder containing numbered subfolders\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_NPZ = \"wlasl_landmarks.npz\"\n",
    "MODEL_OUTPUT = \"wlasl_sequence_model.keras\"\n",
    "LABELS_OUTPUT = \"wlasl_labels.npy\"\n",
    "\n",
    "# Processing parameters\n",
    "SEQUENCE_LENGTH = 32  # Fixed number of frames per sequence\n",
    "FRAME_STRIDE = 2      # Sample every Nth frame\n",
    "MIN_FRAMES = 8        # Discard sequences shorter than this\n",
    "\n",
    "# Gloss selection\n",
    "# Option 1: Specify label IDs (1-300) you want to train on\n",
    "SELECTED_LABEL_IDS = None  # e.g., [1, 5, 10, 25, 50] or None for all\n",
    "\n",
    "# Option 2: If None, use the first N glosses\n",
    "MAX_GLOSSES = 50              # Number of glosses to use if SELECTED_LABEL_IDS is None\n",
    "MAX_SAMPLES_PER_GLOSS = None  # Limit samples per gloss (None = no limit)\n",
    "MIN_VIDEOS_PER_GLOSS = 3      # Minimum videos required per gloss\n",
    "\n",
    "# Training parameters\n",
    "TEST_SIZE = 0.2\n",
    "EPOCHS = 80\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "LSTM_UNITS = [128, 64]\n",
    "DENSE_UNITS = 64\n",
    "DROPOUT = 0.5\n",
    "PATIENCE = 10                 # Early stopping patience\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"\\nLooking for:\")\n",
    "print(f\"  Labels: {LABELS_FILE}\")\n",
    "print(f\"  Videos: {VIDEO_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762608a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5762608a",
    "outputId": "47ede29b-5696-437d-bb72-1f98436b78d3"
   },
   "outputs": [],
   "source": [
    "# Verify that the paths exist\n",
    "import os\n",
    "\n",
    "print(\"Verifying dataset paths...\\n\")\n",
    "\n",
    "# Check labels file\n",
    "if os.path.exists(LABELS_FILE):\n",
    "    print(f\"Found labels file: {LABELS_FILE}\")\n",
    "    with open(LABELS_FILE, 'r') as f:\n",
    "        num_lines = sum(1 for _ in f)\n",
    "    print(f\"  Number of glosses: {num_lines}\")\n",
    "else:\n",
    "    print(f\"Labels file NOT found at: {LABELS_FILE}\")\n",
    "    print(\"\\nSearching for labels.txt...\")\n",
    "    for root, dirs, files in os.walk(\"/content/wlasl_data\"):\n",
    "        if 'labels.txt' in files:\n",
    "            print(f\"  Found: {os.path.join(root, 'labels.txt')}\")\n",
    "    print(\"\\nUpdate LABELS_FILE in the configuration cell above\")\n",
    "\n",
    "# Check video directory\n",
    "print()\n",
    "if os.path.exists(VIDEO_ROOT):\n",
    "    print(f\"Video directory found: {VIDEO_ROOT}\")\n",
    "    subdirs = [d for d in os.listdir(VIDEO_ROOT) if os.path.isdir(os.path.join(VIDEO_ROOT, d)) and d.isdigit()]\n",
    "    print(f\"  Number of gloss folders: {len(subdirs)}\")\n",
    "    if subdirs:\n",
    "        sample_dir = subdirs[0]\n",
    "        sample_path = os.path.join(VIDEO_ROOT, sample_dir)\n",
    "        videos = [f for f in os.listdir(sample_path) if f.endswith('.mp4')]\n",
    "        print(f\"  Sample folder '{sample_dir}' has {len(videos)} videos\")\n",
    "else:\n",
    "    print(f\"Video directory NOT found at: {VIDEO_ROOT}\")\n",
    "    print(\"\\nSearching for WLASL300 folder...\")\n",
    "    for root, dirs, files in os.walk(\"/content/wlasl_data\"):\n",
    "        if 'WLASL300' in dirs:\n",
    "            print(f\"  Found: {os.path.join(root, 'WLASL300')}\")\n",
    "    print(\"\\nUpdate VIDEO_ROOT in the configuration cell above\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if os.path.exists(LABELS_FILE) and os.path.exists(VIDEO_ROOT):\n",
    "    print(\"All paths verified! You can proceed to the next steps.\")\n",
    "else:\n",
    "    print(\"Please update the configuration paths based on the information above.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee32e640",
   "metadata": {
    "id": "ee32e640"
   },
   "source": [
    "### Import Libraries and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0149d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61d0149d",
    "outputId": "8ba2c23e-a8ec-45ea-b814-9fc4641a9ab1"
   },
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "print(\"Importing libraries...\")\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "from typing import List, Sequence\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Constants for Holistic\n",
    "# Pose (33 landmarks * 4 values [x,y,z,vis]) + Left Hand (21*3) + Right Hand (21*3)\n",
    "# 132 + 63 + 63 = 258\n",
    "FEATURE_VECTOR_LEN = 258\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Feature Vector Length set to: {FEATURE_VECTOR_LEN}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f460f6e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f460f6e5",
    "outputId": "462ed548-a6c1-4128-fb84-adfbf4dc281e"
   },
   "outputs": [],
   "source": [
    "# Normalization utility function (from preprocessing_utils.py)\n",
    "def normalize_per_hand(X_arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize landmarks per hand: translate by wrist, scale by wrist->middle_mcp distance.\n",
    "    This ensures consistent spatial representation regardless of hand position/size in frame.\n",
    "    \"\"\"\n",
    "    Xn = X_arr.copy()\n",
    "    if Xn.ndim == 1:\n",
    "        Xn = Xn.reshape(1, -1)\n",
    "    Xn = Xn.reshape(-1, NUM_HANDS, NUM_LANDMARKS, COORDS)\n",
    "\n",
    "    for i in range(Xn.shape[0]):\n",
    "        for h in range(NUM_HANDS):\n",
    "            hand = Xn[i, h]\n",
    "            # Skip if hand is all zeros (no detection)\n",
    "            if np.allclose(hand, 0.0):\n",
    "                continue\n",
    "\n",
    "            # Translate: center on wrist\n",
    "            wrist = hand[WRIST_IDX]\n",
    "            hand[:, :2] -= wrist[:2]  # translate x,y; keep z as-is\n",
    "\n",
    "            # Scale: normalize by wrist->middle_mcp distance on xy plane\n",
    "            ref = hand[MIDDLE_MCP_IDX]\n",
    "            scale = np.linalg.norm(ref[:2])\n",
    "            if scale > 1e-6:\n",
    "                hand[:, :2] /= scale\n",
    "\n",
    "            Xn[i, h] = hand\n",
    "\n",
    "    return Xn.reshape(-1, FEATURE_VECTOR_LEN)\n",
    "\n",
    "print(\"Normalization function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VRvy8NCvosO2",
   "metadata": {
    "id": "VRvy8NCvosO2"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3044e50b",
   "metadata": {
    "id": "3044e50b"
   },
   "source": [
    "### Define Preprocessing Functions\n",
    "\n",
    "These functions handle:\n",
    "- Loading WLASL metadata\n",
    "- Selecting glosses (words) to train on\n",
    "- Building MediaPipe hand detector\n",
    "- Extracting landmarks from video frames\n",
    "- Processing videos into fixed-length sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e754870c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e754870c",
    "outputId": "9f6e8793-b089-43cd-f12e-0c21b7ab979a"
   },
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "def load_labels(labels_path: str) -> dict:\n",
    "    \"\"\"Load labels.txt file.\"\"\"\n",
    "    label_map = {}\n",
    "    with open(labels_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split(maxsplit=1)\n",
    "                if len(parts) == 2:\n",
    "                    label_map[int(parts[0])] = parts[1]\n",
    "    return label_map\n",
    "\n",
    "def scan_dataset(video_root: str, label_map: dict, selected_ids: List[int] = None,\n",
    "                 max_glosses: int = 50, min_videos: int = 3) -> dict:\n",
    "    \"\"\"Scan dataset for valid videos.\"\"\"\n",
    "    glosses_data = {}\n",
    "    ids_to_process = selected_ids if selected_ids else sorted(label_map.keys())[:max_glosses]\n",
    "\n",
    "    for label_id in ids_to_process:\n",
    "        if label_id not in label_map: continue\n",
    "\n",
    "        gloss_name = label_map[label_id]\n",
    "        folder_path = os.path.join(video_root, str(label_id))\n",
    "\n",
    "        if not os.path.exists(folder_path): continue\n",
    "\n",
    "        video_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.mp4')]\n",
    "\n",
    "        if len(video_files) >= min_videos:\n",
    "            glosses_data[gloss_name] = [(label_id, vf) for vf in video_files]\n",
    "\n",
    "    return glosses_data\n",
    "\n",
    "def build_detector(task_path: str):\n",
    "    \"\"\"Initialize MediaPipe Holistic.\"\"\"\n",
    "    # task_path is unused here but kept for compatibility\n",
    "    return mp_holistic.Holistic(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        smooth_landmarks=True,\n",
    "        enable_segmentation=False,\n",
    "        refine_face_landmarks=False\n",
    "    )\n",
    "\n",
    "def extract_landmarks(detector, frame_rgb: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract Pose + Left Hand + Right Hand landmarks.\n",
    "    \"\"\"\n",
    "    results = detector.process(frame_rgb)\n",
    "\n",
    "    # 1. Extract Pose (33 landmarks * 4 values: x, y, z, visibility)\n",
    "    if results.pose_landmarks:\n",
    "        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        pose = np.zeros(33 * 4)\n",
    "\n",
    "    # 2. Extract Left Hand (21 landmarks * 3 values: x, y, z)\n",
    "    if results.left_hand_landmarks:\n",
    "        lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        lh = np.zeros(21 * 3)\n",
    "\n",
    "    # 3. Extract Right Hand (21 landmarks * 3 values: x, y, z)\n",
    "    if results.right_hand_landmarks:\n",
    "        rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        rh = np.zeros(21 * 3)\n",
    "\n",
    "    # Concatenate all features\n",
    "    return np.concatenate([pose, lh, rh]).astype(np.float32)\n",
    "\n",
    "def process_video(detector, video_path: str, sequence_length: int, frame_stride: int) -> np.ndarray:\n",
    "    \"\"\"Process video using Holistic detector.\"\"\"\n",
    "    capture = cv2.VideoCapture(video_path)\n",
    "    if not capture.isOpened():\n",
    "        raise RuntimeError(f\"Could not open video '{video_path}'\")\n",
    "\n",
    "    total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    sampled = []\n",
    "    frame_index = 0\n",
    "\n",
    "    while frame_index < total_frames:\n",
    "        success, frame_bgr = capture.read()\n",
    "        if not success: break\n",
    "\n",
    "        if frame_index % frame_stride == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            # Extract holistic landmarks\n",
    "            features = extract_landmarks(detector, frame_rgb)\n",
    "            sampled.append(features)\n",
    "\n",
    "        frame_index += 1\n",
    "        if len(sampled) >= sequence_length: break\n",
    "\n",
    "    capture.release()\n",
    "\n",
    "    if not sampled:\n",
    "        return np.empty((0, FEATURE_VECTOR_LEN), dtype=np.float32)\n",
    "\n",
    "    sequence = np.stack(sampled)\n",
    "\n",
    "    # Pad or Truncate\n",
    "    if sequence.shape[0] >= sequence_length:\n",
    "        return sequence[:sequence_length]\n",
    "\n",
    "    padding = np.zeros((sequence_length - sequence.shape[0], FEATURE_VECTOR_LEN), dtype=np.float32)\n",
    "    return np.vstack((sequence, padding))\n",
    "\n",
    "print(\"Holistic Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ac8cfa",
   "metadata": {
    "id": "a5ac8cfa"
   },
   "source": [
    "### Run Preprocessing\n",
    "\n",
    "In this section:\n",
    "1. Load WLASL metadata\n",
    "2. Select glosses to train on\n",
    "3. Process each video to extract landmarks\n",
    "4. Save results to `wlasl_landmarks.npz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qoU7uI93cg-6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoU7uI93cg-6",
    "outputId": "62fae836-0ecb-4d32-d148-b38be2bfc0f9"
   },
   "outputs": [],
   "source": [
    "# Temporarily redefine paths for this cell's execution to resolve FileNotFoundError\n",
    "LABELS_FILE = \"/content/wlasl_data/wlasl300_dataset/labels.txt\"\n",
    "VIDEO_ROOT = \"/content/wlasl_data/wlasl300_dataset/WLASL300\"\n",
    "\n",
    "# Load labels\n",
    "print(\"Loading label mappings...\")\n",
    "label_map = load_labels(LABELS_FILE)\n",
    "print(f\"✓ Loaded {len(label_map)} label mappings\")\n",
    "\n",
    "# Scan dataset\n",
    "print(\"\\nScanning dataset...\")\n",
    "glosses_data = scan_dataset(\n",
    "    VIDEO_ROOT,\n",
    "    label_map,\n",
    "    SELECTED_LABEL_IDS,\n",
    "    MAX_GLOSSES,\n",
    "    MIN_VIDEOS_PER_GLOSS\n",
    ")\n",
    "print(f\"✓ Found {len(glosses_data)} glosses meeting criteria\")\n",
    "print(f\"Glosses: {sorted(glosses_data.keys())}\")\n",
    "\n",
    "# Count total videos\n",
    "total_videos = sum(len(videos) for videos in glosses_data.values())\n",
    "print(f\"Total videos to process: {total_videos}\")\n",
    "\n",
    "# Build detector\n",
    "print(\"\\nInitializing MediaPipe hand detector...\")\n",
    "detector = build_detector(\"hand_landmarker.task\")\n",
    "print(\"✓ Detector ready\")\n",
    "\n",
    "# Process videos\n",
    "print(\"\\nProcessing videos...\")\n",
    "sequences = []\n",
    "labels = []\n",
    "video_ids = []\n",
    "per_gloss_counts = Counter()\n",
    "\n",
    "total_processed = 0\n",
    "total_skipped = 0\n",
    "\n",
    "for gloss_name, video_list in glosses_data.items():\n",
    "    for label_id, video_path in video_list:\n",
    "        # Check if we've hit the per-gloss limit\n",
    "        if MAX_SAMPLES_PER_GLOSS and per_gloss_counts[gloss_name] >= MAX_SAMPLES_PER_GLOSS:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            sequence = process_video(\n",
    "                detector=detector,\n",
    "                video_path=video_path,\n",
    "                sequence_length=SEQUENCE_LENGTH,\n",
    "                frame_stride=FRAME_STRIDE,\n",
    "            )\n",
    "        except Exception as exc:\n",
    "            print(f\"⚠ Failed to process {video_path}: {exc}\")\n",
    "            total_skipped += 1\n",
    "            continue\n",
    "\n",
    "        # Check if sequence has enough valid frames\n",
    "        actual_length = np.count_nonzero(np.linalg.norm(sequence, axis=1))\n",
    "        if actual_length < MIN_FRAMES:\n",
    "            total_skipped += 1\n",
    "            continue\n",
    "\n",
    "        sequences.append(sequence)\n",
    "        labels.append(gloss_name)\n",
    "        video_ids.append(os.path.basename(video_path))\n",
    "        per_gloss_counts[gloss_name] += 1\n",
    "        total_processed += 1\n",
    "\n",
    "        # Progress update\n",
    "        if total_processed % 50 == 0:\n",
    "            print(f\"  Processed {total_processed}/{total_videos} videos...\")\n",
    "\n",
    "print(f\"\\n✓ Processing complete!\")\n",
    "print(f\"  Total processed: {total_processed}\")\n",
    "print(f\"  Total skipped: {total_skipped}\")\n",
    "\n",
    "# Save to NPZ\n",
    "if not sequences:\n",
    "    raise RuntimeError(\"No sequences were extracted. Check your paths and dataset.\")\n",
    "\n",
    "X = np.stack(sequences).astype(np.float32)\n",
    "y = np.array(labels)\n",
    "vids = np.array(video_ids)\n",
    "\n",
    "glosses_to_keep = sorted(glosses_data.keys())\n",
    "np.savez(OUTPUT_NPZ, sequences=X, labels=y, video_ids=vids, glosses=np.array(glosses_to_keep))\n",
    "\n",
    "print(f\"\\n✓ Saved {X.shape[0]} samples to {OUTPUT_NPZ}\")\n",
    "print(f\"  Sequence shape: {X.shape}\")\n",
    "print(\"\\n  Sample distribution:\")\n",
    "for gloss in sorted(per_gloss_counts):\n",
    "    print(f\"    {gloss}: {per_gloss_counts[gloss]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca23f9d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca23f9d6",
    "outputId": "eb6924f2-9555-472c-e016-2a0d92f33ab3"
   },
   "outputs": [],
   "source": [
    "# Clean up to free memory\n",
    "import gc\n",
    "\n",
    "print(\"\\nCleaning up memory...\")\n",
    "del detector  # Remove detector object\n",
    "gc.collect()  # Force garbage collection\n",
    "\n",
    "print(\"Memory cleanup complete\")\n",
    "print(\"\\nYou can now proceed to training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6491640d",
   "metadata": {
    "id": "6491640d"
   },
   "source": [
    "### Backup Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461ae1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6461ae1d",
    "outputId": "fa32a2ff-25b9-4817-d849-875632999db5"
   },
   "outputs": [],
   "source": [
    "# Copy preprocessed data to Google Drive for safekeeping\n",
    "import shutil\n",
    "\n",
    "BACKUP_PATH = \"/content/drive/MyDrive/WLASL Dataset/wlasl_landmarks.npz\"\n",
    "\n",
    "try:\n",
    "    shutil.copy(OUTPUT_NPZ, BACKUP_PATH)\n",
    "    print(f\"Backup saved to: {BACKUP_PATH}\")\n",
    "    print(\"\\nIf your session disconnects, you can restore by running:\")\n",
    "    print(f\"  !cp '{BACKUP_PATH}' '{OUTPUT_NPZ}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not backup to Drive: {e}\")\n",
    "    print(\"  Continuing with local copy only...\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
