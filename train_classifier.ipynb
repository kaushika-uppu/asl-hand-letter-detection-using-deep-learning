{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train ASL Classifier\n",
        "\n",
        "This notebook trains a classifier on precomputed MediaPipe hand landmarks stored in `train_landmarks.npz`.\n",
        "\n",
        "- Features: 126-d vectors (2 hands × 21 landmarks × 3 coords)\n",
        "- Labels: 29 classes (A–Z, del, nothing, space)\n",
        "\n",
        "Tip: RBF SVM on 87k samples can be slow. You can switch to a linear model if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install dependencies\n",
        "# %pip install numpy scikit-learn joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Tuple\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.svm import SVC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_dataset(npz_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    data = np.load(npz_path)\n",
        "    X = data[\"hand_landmarks\"].astype(np.float32)\n",
        "    y = data[\"labels\"].astype(str)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def build_model() -> Pipeline:\n",
        "    # Standardize features then train an SVM with RBF kernel\n",
        "    model = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svc\", SVC(kernel=\"rbf\", C=10.0, gamma=\"scale\", probability=True, class_weight=None)),\n",
        "    ])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "DATA_PATH = \"C:/Users/Nivedita/Documents/258/asl-hand-letter-detection-using-deep-learning/train_landmarks.npz\"\n",
        "MODEL_PATH = \"C:/Users/Nivedita/Documents/258/asl-hand-letter-detection-using-deep-learning/asl_landmark_svc.joblib\"\n",
        "TEST_SIZE = 0.15\n",
        "RANDOM_STATE = 42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}\")\n",
        "\n",
        "X, y = load_dataset(DATA_PATH)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "model = build_model()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy: {acc:.4f}\")\n",
        "print(\"Classification report:\\n\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "joblib.dump(model, MODEL_PATH)\n",
        "print(f\"Saved model to {MODEL_PATH}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
