{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fd4qPvKOQNR",
        "outputId": "20b80081-9144-4739-f2f1-ff3d43eecc8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device: cuda\n",
            "Dataset already extracted.\n",
            "\n",
            "Loading CLIP Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing train set...\n",
            "  > Pass 1/4 (Augment=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [03:30<00:00,  2.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  > Pass 2/4 (Augment=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [04:22<00:00,  2.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  > Pass 3/4 (Augment=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [04:21<00:00,  2.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  > Pass 4/4 (Augment=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [04:20<00:00,  2.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing val set...\n",
            "  > Pass 1/1 (Augment=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:44<00:00,  2.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing test set...\n",
            "  > Pass 1/1 (Augment=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:33<00:00,  2.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Dataset Sizes:\n",
            "Train: (5760, 32, 768) (Augmented x4)\n",
            "Val:   (337, 32, 768)\n",
            "Test:  (258, 32, 768)\n",
            "✅ Backup Saved to Drive.\n",
            "\n",
            "Preparing Training...\n",
            "Starting Training...\n",
            "Epoch 1/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 47ms/step - accuracy: 0.0107 - loss: 5.5555 - top5: 0.0571 - val_accuracy: 0.0208 - val_loss: 4.6565 - val_top5: 0.0712 - learning_rate: 1.0000e-04\n",
            "Epoch 2/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0265 - loss: 5.0717 - top5: 0.0942 - val_accuracy: 0.0326 - val_loss: 4.6255 - val_top5: 0.1187 - learning_rate: 1.0000e-04\n",
            "Epoch 3/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0459 - loss: 4.7731 - top5: 0.1561 - val_accuracy: 0.0534 - val_loss: 4.6853 - val_top5: 0.1484 - learning_rate: 1.0000e-04\n",
            "Epoch 4/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1023 - loss: 4.2839 - top5: 0.2805 - val_accuracy: 0.0920 - val_loss: 4.4022 - val_top5: 0.2374 - learning_rate: 1.0000e-04\n",
            "Epoch 5/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1835 - loss: 3.7389 - top5: 0.4334 - val_accuracy: 0.1276 - val_loss: 4.1756 - val_top5: 0.3205 - learning_rate: 1.0000e-04\n",
            "Epoch 6/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2322 - loss: 3.3955 - top5: 0.5406 - val_accuracy: 0.1632 - val_loss: 3.9885 - val_top5: 0.3858 - learning_rate: 1.0000e-04\n",
            "Epoch 7/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3253 - loss: 3.0308 - top5: 0.6506 - val_accuracy: 0.1840 - val_loss: 3.8145 - val_top5: 0.4451 - learning_rate: 1.0000e-04\n",
            "Epoch 8/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3800 - loss: 2.7709 - top5: 0.7176 - val_accuracy: 0.2463 - val_loss: 3.7038 - val_top5: 0.4748 - learning_rate: 1.0000e-04\n",
            "Epoch 9/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4532 - loss: 2.5374 - top5: 0.7931 - val_accuracy: 0.2611 - val_loss: 3.6122 - val_top5: 0.5045 - learning_rate: 1.0000e-04\n",
            "Epoch 10/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5001 - loss: 2.3867 - top5: 0.8316 - val_accuracy: 0.2789 - val_loss: 3.5445 - val_top5: 0.5074 - learning_rate: 1.0000e-04\n",
            "Epoch 11/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5444 - loss: 2.2565 - top5: 0.8571 - val_accuracy: 0.2819 - val_loss: 3.4889 - val_top5: 0.5282 - learning_rate: 1.0000e-04\n",
            "Epoch 12/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5888 - loss: 2.1050 - top5: 0.8926 - val_accuracy: 0.2700 - val_loss: 3.4673 - val_top5: 0.5638 - learning_rate: 1.0000e-04\n",
            "Epoch 13/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6336 - loss: 2.0048 - top5: 0.9146 - val_accuracy: 0.2967 - val_loss: 3.4248 - val_top5: 0.5816 - learning_rate: 1.0000e-04\n",
            "Epoch 14/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6612 - loss: 1.9288 - top5: 0.9261 - val_accuracy: 0.3086 - val_loss: 3.4120 - val_top5: 0.5549 - learning_rate: 1.0000e-04\n",
            "Epoch 15/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7133 - loss: 1.7890 - top5: 0.9509 - val_accuracy: 0.2997 - val_loss: 3.4088 - val_top5: 0.5786 - learning_rate: 1.0000e-04\n",
            "Epoch 16/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7245 - loss: 1.7652 - top5: 0.9497 - val_accuracy: 0.2789 - val_loss: 3.4307 - val_top5: 0.5638 - learning_rate: 1.0000e-04\n",
            "Epoch 17/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7558 - loss: 1.6760 - top5: 0.9667 - val_accuracy: 0.3175 - val_loss: 3.3755 - val_top5: 0.5875 - learning_rate: 1.0000e-04\n",
            "Epoch 18/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7766 - loss: 1.6164 - top5: 0.9776 - val_accuracy: 0.3294 - val_loss: 3.3408 - val_top5: 0.6350 - learning_rate: 1.0000e-04\n",
            "Epoch 19/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7979 - loss: 1.5617 - top5: 0.9786 - val_accuracy: 0.3412 - val_loss: 3.3297 - val_top5: 0.6231 - learning_rate: 1.0000e-04\n",
            "Epoch 20/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8199 - loss: 1.4996 - top5: 0.9816 - val_accuracy: 0.3472 - val_loss: 3.3418 - val_top5: 0.6142 - learning_rate: 1.0000e-04\n",
            "Epoch 21/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8378 - loss: 1.4682 - top5: 0.9892 - val_accuracy: 0.3383 - val_loss: 3.3409 - val_top5: 0.6409 - learning_rate: 1.0000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8371 - loss: 1.4443 - top5: 0.9880 - val_accuracy: 0.3472 - val_loss: 3.3131 - val_top5: 0.6380 - learning_rate: 1.0000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8660 - loss: 1.3952 - top5: 0.9908 - val_accuracy: 0.3442 - val_loss: 3.3593 - val_top5: 0.6350 - learning_rate: 1.0000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8845 - loss: 1.3662 - top5: 0.9954 - val_accuracy: 0.3501 - val_loss: 3.2844 - val_top5: 0.6320 - learning_rate: 1.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8918 - loss: 1.3364 - top5: 0.9961 - val_accuracy: 0.3561 - val_loss: 3.2896 - val_top5: 0.6380 - learning_rate: 1.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8958 - loss: 1.3174 - top5: 0.9956 - val_accuracy: 0.3887 - val_loss: 3.2965 - val_top5: 0.6409 - learning_rate: 1.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9068 - loss: 1.2860 - top5: 0.9954 - val_accuracy: 0.3620 - val_loss: 3.3112 - val_top5: 0.6350 - learning_rate: 1.0000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9108 - loss: 1.2702 - top5: 0.9967 - val_accuracy: 0.3858 - val_loss: 3.2750 - val_top5: 0.6320 - learning_rate: 1.0000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9229 - loss: 1.2524 - top5: 0.9980 - val_accuracy: 0.3650 - val_loss: 3.2913 - val_top5: 0.6231 - learning_rate: 1.0000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9253 - loss: 1.2415 - top5: 0.9985 - val_accuracy: 0.3680 - val_loss: 3.2756 - val_top5: 0.6409 - learning_rate: 1.0000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9392 - loss: 1.2113 - top5: 0.9981 - val_accuracy: 0.3887 - val_loss: 3.2726 - val_top5: 0.6320 - learning_rate: 1.0000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9499 - loss: 1.1818 - top5: 0.9977 - val_accuracy: 0.3828 - val_loss: 3.2355 - val_top5: 0.6320 - learning_rate: 5.0000e-05\n",
            "Epoch 33/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9586 - loss: 1.1644 - top5: 0.9996 - val_accuracy: 0.3858 - val_loss: 3.2393 - val_top5: 0.6469 - learning_rate: 5.0000e-05\n",
            "Epoch 34/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9620 - loss: 1.1559 - top5: 0.9999 - val_accuracy: 0.3709 - val_loss: 3.2643 - val_top5: 0.6439 - learning_rate: 5.0000e-05\n",
            "Epoch 35/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9674 - loss: 1.1433 - top5: 0.9998 - val_accuracy: 0.3887 - val_loss: 3.2428 - val_top5: 0.6202 - learning_rate: 5.0000e-05\n",
            "Epoch 36/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9667 - loss: 1.1392 - top5: 0.9991 - val_accuracy: 0.3798 - val_loss: 3.2517 - val_top5: 0.6142 - learning_rate: 5.0000e-05\n",
            "Epoch 37/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9703 - loss: 1.1217 - top5: 0.9998 - val_accuracy: 0.3798 - val_loss: 3.2236 - val_top5: 0.6380 - learning_rate: 2.5000e-05\n",
            "Epoch 38/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9785 - loss: 1.1072 - top5: 1.0000 - val_accuracy: 0.3917 - val_loss: 3.2184 - val_top5: 0.6380 - learning_rate: 2.5000e-05\n",
            "Epoch 39/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9804 - loss: 1.1056 - top5: 1.0000 - val_accuracy: 0.3798 - val_loss: 3.2302 - val_top5: 0.6558 - learning_rate: 2.5000e-05\n",
            "Epoch 40/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9765 - loss: 1.1066 - top5: 0.9997 - val_accuracy: 0.3917 - val_loss: 3.2181 - val_top5: 0.6439 - learning_rate: 2.5000e-05\n",
            "Epoch 41/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9776 - loss: 1.1040 - top5: 1.0000 - val_accuracy: 0.3828 - val_loss: 3.2429 - val_top5: 0.6202 - learning_rate: 2.5000e-05\n",
            "Epoch 42/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9790 - loss: 1.0956 - top5: 0.9996 - val_accuracy: 0.3739 - val_loss: 3.2515 - val_top5: 0.6439 - learning_rate: 2.5000e-05\n",
            "Epoch 43/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9771 - loss: 1.0991 - top5: 1.0000 - val_accuracy: 0.3828 - val_loss: 3.2278 - val_top5: 0.6320 - learning_rate: 2.5000e-05\n",
            "Epoch 44/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9785 - loss: 1.0938 - top5: 0.9998 - val_accuracy: 0.3887 - val_loss: 3.2067 - val_top5: 0.6439 - learning_rate: 1.2500e-05\n",
            "Epoch 45/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9782 - loss: 1.0971 - top5: 0.9996 - val_accuracy: 0.3976 - val_loss: 3.1932 - val_top5: 0.6469 - learning_rate: 1.2500e-05\n",
            "Epoch 46/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9814 - loss: 1.0814 - top5: 1.0000 - val_accuracy: 0.3917 - val_loss: 3.2069 - val_top5: 0.6439 - learning_rate: 1.2500e-05\n",
            "Epoch 47/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9812 - loss: 1.0862 - top5: 1.0000 - val_accuracy: 0.3887 - val_loss: 3.1957 - val_top5: 0.6469 - learning_rate: 1.2500e-05\n",
            "Epoch 48/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9852 - loss: 1.0826 - top5: 0.9996 - val_accuracy: 0.3917 - val_loss: 3.2055 - val_top5: 0.6469 - learning_rate: 1.2500e-05\n",
            "Epoch 49/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9852 - loss: 1.0840 - top5: 1.0000 - val_accuracy: 0.3947 - val_loss: 3.1909 - val_top5: 0.6439 - learning_rate: 1.2500e-05\n",
            "Epoch 50/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 1.0725 - top5: 1.0000 - val_accuracy: 0.3947 - val_loss: 3.1887 - val_top5: 0.6469 - learning_rate: 1.2500e-05\n",
            "Epoch 51/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9839 - loss: 1.0763 - top5: 0.9998 - val_accuracy: 0.3947 - val_loss: 3.1835 - val_top5: 0.6380 - learning_rate: 6.2500e-06\n",
            "Epoch 52/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9827 - loss: 1.0734 - top5: 1.0000 - val_accuracy: 0.3947 - val_loss: 3.1949 - val_top5: 0.6380 - learning_rate: 6.2500e-06\n",
            "Epoch 53/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9875 - loss: 1.0738 - top5: 1.0000 - val_accuracy: 0.3947 - val_loss: 3.1835 - val_top5: 0.6409 - learning_rate: 6.2500e-06\n",
            "Epoch 54/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9815 - loss: 1.0744 - top5: 0.9999 - val_accuracy: 0.4006 - val_loss: 3.1834 - val_top5: 0.6380 - learning_rate: 6.2500e-06\n",
            "Epoch 55/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9843 - loss: 1.0760 - top5: 0.9998 - val_accuracy: 0.4065 - val_loss: 3.1803 - val_top5: 0.6439 - learning_rate: 6.2500e-06\n",
            "Epoch 56/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9852 - loss: 1.0726 - top5: 0.9999 - val_accuracy: 0.3947 - val_loss: 3.1817 - val_top5: 0.6469 - learning_rate: 6.2500e-06\n",
            "Epoch 57/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9872 - loss: 1.0675 - top5: 1.0000 - val_accuracy: 0.3976 - val_loss: 3.1865 - val_top5: 0.6499 - learning_rate: 6.2500e-06\n",
            "Epoch 58/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9860 - loss: 1.0688 - top5: 0.9996 - val_accuracy: 0.3947 - val_loss: 3.1881 - val_top5: 0.6409 - learning_rate: 6.2500e-06\n",
            "Epoch 59/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9850 - loss: 1.0731 - top5: 0.9999 - val_accuracy: 0.4036 - val_loss: 3.1875 - val_top5: 0.6439 - learning_rate: 6.2500e-06\n",
            "Epoch 60/60\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9893 - loss: 1.0712 - top5: 1.0000 - val_accuracy: 0.4006 - val_loss: 3.1793 - val_top5: 0.6528 - learning_rate: 6.2500e-06\n",
            "\n",
            "Final Evaluation:\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 471ms/step - accuracy: 0.3794 - loss: 3.2009 - top5: 0.6443\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.389517307281494, 0.3139534890651703, 0.604651153087616]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. SETUP & INSTALLATION\n",
        "# ==========================================\n",
        "!pip install -q transformers torch opencv-python-headless tqdm scikit-learn\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import zipfile\n",
        "import torch\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from transformers import CLIPProcessor, CLIPVisionModel\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ==========================================\n",
        "# 2. CONFIGURATION\n",
        "# ==========================================\n",
        "# Input Path\n",
        "DRIVE_ZIP_PATH = \"/content/drive/MyDrive/WLASL100 Dataset/archive.zip\"\n",
        "\n",
        "# Extraction Target\n",
        "# Extracts to: /content/preprocessing/train, /content/preprocessing/val, etc.\n",
        "EXTRACT_ROOT = \"/content/preprocessing\"\n",
        "\n",
        "# Output Paths\n",
        "OUTPUT_FILENAME = \"wlasl100_augmented_features.npz\"\n",
        "BACKUP_PATH = \"/content/drive/MyDrive/WLASL100 Dataset/wlasl100_augmented_features.npz\"\n",
        "\n",
        "# Processing Config\n",
        "SEQUENCE_LENGTH = 32\n",
        "BATCH_SIZE = 32\n",
        "AUGMENT_COPIES = 3  # How many extra augmented copies to generate for training\n",
        "\n",
        "# Setup GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using Device: {device}\")\n",
        "if device == \"cpu\":\n",
        "    print(\"⚠️ WARNING: Switch to T4 GPU Runtime for speed!\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. EXTRACT DATASET\n",
        "# ==========================================\n",
        "if not os.path.exists(EXTRACT_ROOT):\n",
        "    print(f\"Extracting {DRIVE_ZIP_PATH}...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(DRIVE_ZIP_PATH, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"/content\") # Creates /content/preprocessing\n",
        "        print(\"✅ Extraction Complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error extracting: {e}\")\n",
        "else:\n",
        "    print(\"Dataset already extracted.\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. AUGMENTATION & CLIP MODEL\n",
        "# ==========================================\n",
        "print(\"\\nLoading CLIP Model...\")\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "vision_model = CLIPVisionModel.from_pretrained(model_name).to(device)\n",
        "vision_model.eval()\n",
        "\n",
        "def augment_frames(frames):\n",
        "    \"\"\"Applies random visual noise to a sequence of frames.\"\"\"\n",
        "    augmented = []\n",
        "\n",
        "    # Random Params (Consistent across the whole video)\n",
        "    h, w, _ = frames[0].shape\n",
        "\n",
        "    # 1. Random Crop (Zoom in 85% - 100%)\n",
        "    scale = random.uniform(0.85, 1.0)\n",
        "    new_h, new_w = int(h * scale), int(w * scale)\n",
        "    y_start = random.randint(0, h - new_h)\n",
        "    x_start = random.randint(0, w - new_w)\n",
        "\n",
        "    # 2. Random Rotation (-10 to +10 degrees)\n",
        "    angle = random.uniform(-10, 10)\n",
        "    center = (w // 2, h // 2)\n",
        "    rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "\n",
        "    # 3. Brightness\n",
        "    brightness = random.uniform(0.7, 1.3)\n",
        "\n",
        "    for frame in frames:\n",
        "        # Crop & Resize\n",
        "        crop = frame[y_start:y_start+new_h, x_start:x_start+new_w]\n",
        "        crop = cv2.resize(crop, (w, h))\n",
        "\n",
        "        # Rotate\n",
        "        rotated = cv2.warpAffine(crop, rot_mat, (w, h))\n",
        "\n",
        "        # Adjust Brightness\n",
        "        img_float = rotated.astype(np.float32) * brightness\n",
        "        img_final = np.clip(img_float, 0, 255).astype(np.uint8)\n",
        "\n",
        "        augmented.append(img_final)\n",
        "\n",
        "    return augmented\n",
        "\n",
        "def process_video_frames(frame_files, augment=False):\n",
        "    \"\"\"Reads, (optionally) augments, and extracts features.\"\"\"\n",
        "    frame_files.sort()\n",
        "    if len(frame_files) == 0: return None\n",
        "\n",
        "    # Uniform Sampling\n",
        "    indices = np.linspace(0, len(frame_files) - 1, SEQUENCE_LENGTH, dtype=int)\n",
        "    selected_files = [frame_files[i] for i in indices]\n",
        "\n",
        "    images = []\n",
        "    for f in selected_files:\n",
        "        img = cv2.imread(f)\n",
        "        if img is not None:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            images.append(img)\n",
        "\n",
        "    if not images: return None\n",
        "\n",
        "    # === AUGMENTATION ===\n",
        "    if augment:\n",
        "        images = augment_frames(images)\n",
        "    # ====================\n",
        "\n",
        "    # Pad if short\n",
        "    while len(images) < SEQUENCE_LENGTH:\n",
        "        images.append(images[-1])\n",
        "\n",
        "    try:\n",
        "        inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = vision_model(**inputs)\n",
        "            embeddings = outputs.pooler_output.cpu().numpy()\n",
        "        return embeddings.astype(np.float32)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def extract_split(split_name, augment_count=0):\n",
        "    \"\"\"\n",
        "    Extracts features for a split.\n",
        "    If augment_count > 0, generates multiple copies of the data.\n",
        "    \"\"\"\n",
        "    # Path: /content/preprocessing/train/frames\n",
        "    frames_root = os.path.join(EXTRACT_ROOT, split_name, \"frames\")\n",
        "    if not os.path.exists(frames_root):\n",
        "        print(f\"❌ Split not found: {frames_root}\")\n",
        "        return [], []\n",
        "\n",
        "    classes = sorted(os.listdir(frames_root))\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    print(f\"Processing {split_name} set...\")\n",
        "\n",
        "    # Calculate total iterations for progress bar\n",
        "    total_passes = 1 + augment_count\n",
        "\n",
        "    for i in range(total_passes):\n",
        "        is_aug = (i > 0) # First pass is original, others are augmented\n",
        "        print(f\"  > Pass {i+1}/{total_passes} (Augment={is_aug})\")\n",
        "\n",
        "        for class_name in tqdm(classes):\n",
        "            class_path = os.path.join(frames_root, class_name)\n",
        "            if not os.path.isdir(class_path): continue\n",
        "\n",
        "            # Find video folders\n",
        "            items = os.listdir(class_path)\n",
        "            # Typically: class_path/video_id/*.jpg\n",
        "            subfolders = [os.path.join(class_path, item) for item in items if os.path.isdir(os.path.join(class_path, item))]\n",
        "\n",
        "            for video_folder in subfolders:\n",
        "                images = [os.path.join(video_folder, f) for f in os.listdir(video_folder) if f.endswith('.jpg')]\n",
        "\n",
        "                features = process_video_frames(images, augment=is_aug)\n",
        "\n",
        "                if features is not None:\n",
        "                    X_list.append(features)\n",
        "                    y_list.append(class_name)\n",
        "\n",
        "    return np.array(X_list), np.array(y_list)\n",
        "\n",
        "# ==========================================\n",
        "# 5. EXECUTE EXTRACTION\n",
        "# ==========================================\n",
        "# Train: 1 Original + 3 Augmented copies\n",
        "X_train, y_train_raw = extract_split(\"train\", augment_count=AUGMENT_COPIES)\n",
        "\n",
        "# Val/Test: No augmentation (evaluate on real data)\n",
        "X_val, y_val_raw = extract_split(\"val\", augment_count=0)\n",
        "X_test, y_test_raw = extract_split(\"test\", augment_count=0)\n",
        "\n",
        "print(f\"\\nFinal Dataset Sizes:\")\n",
        "print(f\"Train: {X_train.shape} (Augmented x{AUGMENT_COPIES+1})\")\n",
        "print(f\"Val:   {X_val.shape}\")\n",
        "print(f\"Test:  {X_test.shape}\")\n",
        "\n",
        "# Backup\n",
        "np.savez(OUTPUT_FILENAME, X_train=X_train, y_train=y_train_raw,\n",
        "         X_val=X_val, y_val=y_val_raw, X_test=X_test, y_test=y_test_raw)\n",
        "try:\n",
        "    shutil.copy(OUTPUT_FILENAME, BACKUP_PATH)\n",
        "    print(\"✅ Backup Saved to Drive.\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# ==========================================\n",
        "# 6. TRAINING (Regularized Model)\n",
        "# ==========================================\n",
        "print(\"\\nPreparing Training...\")\n",
        "\n",
        "# One-Hot Encoding\n",
        "le = LabelEncoder()\n",
        "le.fit(y_train_raw)\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "y_train = to_categorical(le.transform(y_train_raw), num_classes)\n",
        "y_val = to_categorical(le.transform(y_val_raw), num_classes)\n",
        "y_test = to_categorical(le.transform(y_test_raw), num_classes)\n",
        "\n",
        "# Datasets\n",
        "def create_ds(X, y, shuffle=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    if shuffle: ds = ds.shuffle(len(X))\n",
        "    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = create_ds(X_train, y_train, shuffle=True)\n",
        "val_ds = create_ds(X_val, y_val)\n",
        "test_ds = create_ds(X_test, y_test)\n",
        "\n",
        "# Model Architecture (High Regularization)\n",
        "def build_regularized_model(num_classes):\n",
        "    inputs = layers.Input(shape=(SEQUENCE_LENGTH, 768))\n",
        "\n",
        "    # 1. Smaller Projection with High Dropout\n",
        "    x = layers.Dense(256, activation=\"linear\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    # Positional Encoding\n",
        "    positions = tf.range(start=0, limit=SEQUENCE_LENGTH, delta=1)\n",
        "    x = x + layers.Embedding(input_dim=SEQUENCE_LENGTH, output_dim=256)(positions)\n",
        "\n",
        "    # 2. Shallower Transformer (2 Layers)\n",
        "    for _ in range(2):\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        # Attention\n",
        "        att = layers.MultiHeadAttention(num_heads=4, key_dim=64, dropout=0.5)(x1, x1)\n",
        "        x2 = layers.Add()([x, att])\n",
        "\n",
        "        # Feed Forward\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = layers.Dense(512, activation=\"gelu\")(x3)\n",
        "        x3 = layers.Dropout(0.5)(x3)\n",
        "        x3 = layers.Dense(256)(x3)\n",
        "        x = layers.Add()([x2, x3])\n",
        "\n",
        "    # 3. Head\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=\"UniSign_Regularized\")\n",
        "\n",
        "model = build_regularized_model(num_classes)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-4),\n",
        "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5')]\n",
        ")\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=60,  # Increased epochs since data is harder to learn now\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor='val_accuracy'),\n",
        "        keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5, monitor='val_accuracy', min_lr=1e-6)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"\\nFinal Evaluation:\")\n",
        "model.evaluate(test_ds)"
      ]
    }
  ]
}